{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Ratings Analysis\n",
        "\n",
        "This project involves creating a recommendation system using a dataset of movie ratings. The dataset includes ratings for six popular movies by five different users.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ljtH63AtglGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Load the Ratings into a Pandas DataFrame and Save to CSV\n",
        "\n",
        "First, we'll create a DataFrame to store the user ratings for each movie. After creating this DataFrame, we'll save the data to a CSV file for persistent storage and future use.\n",
        "\n",
        "The data includes ratings for six movies by five different users. The ratings are on a scale of 1 to 5, with `None` indicating that a user has not rated that particular movie.\n"
      ],
      "metadata": {
        "id": "Hcra8gVQfYVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UA7hoV1Ae_p-",
        "outputId": "984af8d0-b882-4d81-da4c-a4ca9490a03f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'movies_ratings.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Let's create the initial DataFrame again to ensure we have the data\n",
        "import pandas as pd\n",
        "\n",
        "# Given ratings data\n",
        "ratings_data = {\n",
        "    'American Sniper': [5, 4, None, 3, 5],\n",
        "    'Edge of Tomorrow': [4, None, 4, 2, 4],\n",
        "    'Groundhog Day': [3, 4, None, 2, None],\n",
        "    'Jurassic World': [None, 3, 4, 2, 3],\n",
        "    'Lost in Translation': [None, 4, 4, 3, None],\n",
        "    'Lucy': [4, 4, None, 3, 3]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df_ratings = pd.DataFrame(ratings_data, index=['John', 'Logan', 'Modesto', 'Malcolm', 'Maurice']).transpose()\n",
        "\n",
        "# Now let's save this DataFrame to a CSV file\n",
        "csv_file_path = 'movies_ratings.csv'  # Define the file path\n",
        "df_ratings.to_csv(csv_file_path)  # Save to CSV\n",
        "\n",
        "csv_file_path  # Output the file path for download or reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Calculate Average Ratings\n",
        "We then compute the average ratings for each movie and each user.\n"
      ],
      "metadata": {
        "id": "WDbopauIfmdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average ratings for each user (column-wise average, skipping NaN values)\n",
        "average_ratings_per_user = df_ratings.mean(axis=0)\n",
        "\n",
        "# Calculate the average ratings for each movie (row-wise average, skipping NaN values)\n",
        "average_ratings_per_movie = df_ratings.mean(axis=1)\n",
        "\n",
        "average_ratings_per_user, average_ratings_per_movie\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfnKUSdEgKKf",
        "outputId": "79259d81-46e0-486f-989b-f5615dc1358a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(John       4.00\n",
              " Logan      3.80\n",
              " Modesto    4.00\n",
              " Malcolm    2.50\n",
              " Maurice    3.75\n",
              " dtype: float64,\n",
              " American Sniper        4.250000\n",
              " Edge of Tomorrow       3.500000\n",
              " Groundhog Day          3.000000\n",
              " Jurassic World         3.000000\n",
              " Lost in Translation    3.666667\n",
              " Lucy                   3.500000\n",
              " dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Normalize Ratings\n",
        "To compare ratings on a consistent scale, we normalize the ratings using the Min-Max scaling technique.\n"
      ],
      "metadata": {
        "id": "Db-F7kVefoDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization of ratings (Min-Max scaling)\n",
        "# Normalized_rating = (Rating - Min_rating) / (Max_rating - Min_rating)\n",
        "\n",
        "# Define the normalization function\n",
        "def normalize(ratings):\n",
        "    min_rating = ratings.min(skipna=True)\n",
        "    max_rating = ratings.max(skipna=True)\n",
        "    normalized_ratings = (ratings - min_rating) / (max_rating - min_rating)\n",
        "    return normalized_ratings\n",
        "\n",
        "# Apply the normalization function to each user's ratings (column-wise)\n",
        "normalized_df = df_ratings.apply(normalize, axis=0)\n",
        "\n",
        "# Show the normalized dataframe and calculate the average of the normalized ratings per user\n",
        "normalized_df, normalized_df.mean(axis=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGdQLDPggNPa",
        "outputId": "3e3b0263-1e88-47f5-fa1f-19d2009437c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                     John  Logan  Modesto  Malcolm  Maurice\n",
              " American Sniper       1.0    1.0      NaN      1.0      1.0\n",
              " Edge of Tomorrow      0.5    NaN      NaN      0.0      0.5\n",
              " Groundhog Day         0.0    1.0      NaN      0.0      NaN\n",
              " Jurassic World        NaN    0.0      NaN      0.0      0.0\n",
              " Lost in Translation   NaN    1.0      NaN      1.0      NaN\n",
              " Lucy                  0.5    1.0      NaN      1.0      0.0,\n",
              " John       0.500\n",
              " Logan      0.800\n",
              " Modesto      NaN\n",
              " Malcolm    0.500\n",
              " Maurice    0.375\n",
              " dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Standardize Ratings\n",
        "Next, we standardize the ratings to have a mean of 0 and a standard deviation of 1. This is often a prerequisite for many machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "lzkOopxLfr-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization of ratings (Z-score normalization)\n",
        "# Standardized_rating = (Rating - Mean_rating) / Std_rating\n",
        "\n",
        "# Define the standardization function\n",
        "def standardize(ratings):\n",
        "    mean_rating = ratings.mean(skipna=True)\n",
        "    std_rating = ratings.std(skipna=True)\n",
        "    standardized_ratings = (ratings - mean_rating) / std_rating\n",
        "    return standardized_ratings\n",
        "\n",
        "# Apply the standardization function to each user's ratings (column-wise)\n",
        "standardized_df = df_ratings.apply(standardize, axis=0)\n",
        "\n",
        "# Show the standardized dataframe and calculate the average of the standardized ratings per user\n",
        "standardized_df, standardized_df.mean(axis=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYv1txDugQvk",
        "outputId": "6598a225-4ecd-4a54-ff53-3441887d7dd4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                         John     Logan  Modesto   Malcolm   Maurice\n",
              " American Sniper      1.224745  0.447214      NaN  0.912871  1.305582\n",
              " Edge of Tomorrow     0.000000       NaN      NaN -0.912871  0.261116\n",
              " Groundhog Day       -1.224745  0.447214      NaN -0.912871       NaN\n",
              " Jurassic World            NaN -1.788854      NaN -0.912871 -0.783349\n",
              " Lost in Translation       NaN  0.447214      NaN  0.912871       NaN\n",
              " Lucy                 0.000000  0.447214      NaN  0.912871 -0.783349,\n",
              " John       0.000000e+00\n",
              " Logan      3.996803e-16\n",
              " Modesto             NaN\n",
              " Malcolm    0.000000e+00\n",
              " Maurice   -5.551115e-17\n",
              " dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion on Normalization vs Standardization\n",
        "Normalization and standardization of data are crucial pre-processing steps in data analysis. Here we discuss their advantages and potential drawbacks.\n"
      ],
      "metadata": {
        "id": "o34oRgWNftoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using normalized and standardized ratings in a recommendation system, there are several advantages and disadvantages to consider:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1.  Comparability: Normalization puts all ratings on the same scale, typically 0 to 1, which allows for comparison across different users or items regardless of the original scale.\n",
        "\n",
        "2.  Fairness: It accounts for differences in user rating behavior. Some users may tend to give higher ratings in general (easy raters), while others might give lower ratings (tough raters). Normalization and standardization adjust for these biases.\n",
        "\n",
        "3.  Algorithm Readiness: Many machine learning algorithms expect features to be on a similar scale for them to work correctly. Normalization and standardization are preprocessing steps to make data compatible with these algorithms.\n",
        "\n",
        "4.  Outlier Management: Standardization reduces the impact of outliers since the number of standard deviations away from the mean is more informative than the raw score.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "1.  Loss of Interpretability: The original meaning of the ratings is lost, which can make it harder to interpret the data without the context of the original scale.\n",
        "\n",
        "2.  Data Skewness: If the data has a skewed distribution, standardization might not be the best approach as it assumes the data is normally distributed around the mean.\n",
        "\n",
        "3.  Sensitivity to New Data: Normalized and standardized ratings need to be recalculated when new data comes in, which can be computationally intensive for large datasets.\n",
        "\n",
        "4.  Ignores Null Values: If a user has not rated an item, normalization and standardization do not account for this, which could imply that a non-rating is the same as an average rating.\n",
        "\n",
        "5.  Dependency on the Dataset: The normalized and standardized values are dependent on the dataset. If the dataset changes, such as adding more users or items, the scales may shift, and previously scaled values may no longer be valid.\n",
        "\n",
        "In summary, normalization and standardization are powerful techniques for preprocessing data for recommendation systems, but they require careful implementation and regular updates to maintain their effectiveness. They make the ratings from different users more comparable and can improve the performance of machine learning models, but they also add a layer of complexity and can reduce the clarity of the raw data."
      ],
      "metadata": {
        "id": "A3zxIIFygXeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Credit: Calculate the Average of the Standardized Ratings\n",
        "As an extra step, we calculate the average of the standardized ratings for each movie to see how they compare against each other.\n"
      ],
      "metadata": {
        "id": "LOfVgwCXfx1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the standardized ratings for each movie (row-wise average, skipping NaN values)\n",
        "average_standardized_per_movie = standardized_df.mean(axis=1)\n",
        "\n",
        "average_standardized_per_movie\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upG-TxPNgd2C",
        "outputId": "c1aa9d9c-79d3-4e37-fae5-db71a19c0286"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "American Sniper        0.972603\n",
              "Edge of Tomorrow      -0.217251\n",
              "Groundhog Day         -0.563467\n",
              "Jurassic World        -1.161692\n",
              "Lost in Translation    0.680042\n",
              "Lucy                   0.144184\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In this project, we performed data manipulation to prepare our movie ratings data for a recommendation system. We covered several data processing techniques, including normalization and standardization, which are essential for unbiased analysis and model training.\n"
      ],
      "metadata": {
        "id": "1EIstSSqfzZd"
      }
    }
  ]
}